---
title: "Module 9 Assignment on Unsupervised Methods: PC and Clustering"
author: "First Name + Last Name // Undergraduate/Graduate Student"
date: "Today's date"
#output: pdf_document
output:
  pdf_document: default
  df_print: paged
  #html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=80))
```

***

## Module Assignment Questions
## *Applications*

You will perform four unsupervised methods on a high dimensional data: `PCA`, `K-Means` clustering, `hierarchical` clustering, and one of `DBSCAN` and `GMM` clusterings. The data is the `NCI60` cancer cell line microarray data set, which consists of 6,830 gene expression measurements on 64 cancer cell lines. Each cell line is labeled with a cancer type: there is 14 imbalanced types. In performing unsupervised methods, we don't use labels. But after performing the clustering, we can check to see the extent to which these cancer types agree with the results of these unsupervised techniques. You will do this as well.

Do scaling before performing any unsupervised methods. Then, apply each method by justifying how you use and by including informative plots and summaries: each method has parameters and hyperparameters, consider these. Show how you decide optimal numbers of clusters. There is no unique answer key: any decision should be justified as long as you reflect our lab discussions and details. Don't include irrelevant and uncommented results. Make write-ups and outputs readable and compact. Include only necessary codes and outputs in minimalist format.

Fit the four models (1-4) below by including narratives in terms of data reduction and clustering, and answer the questions (5, 6):

1. PCA

2. K-Means

3. Hierarchical

4. DBSCAN or GMM

5. Do the comparison of the four methods above in a table by fitted clusters and true clusters: did clustering methods discover correct clusters? Include an accuracy table or a graph that compares the results obtained from the four methods. Comment.

6. What insights/contextual conclusions did you get about the data from the PCA application? Explain. (this may overlap with your PCA narratives, here, be more contextual on the results of PCA in determining clusters of cancer types.)

BONUS. Use any `manifold` method to cluster the data in terms of cancer types. Then check with the true labels. Does it discover? Explain and include graphs.

***
\newpage

## Your Solutions

```{r}
library(MASS)
library(ISLR)
```

```{r}
attach(NCI60)
```


```{r}
summary(NCI60)
#View(NCI60$data)
#View(NCI60$labs)
```

```{r}
nci.labs=NCI60$labs
nci.data=NCI60$data
nci.labs=NCI60$labs
```

```{r}
other.labs <- c("UNKNOWN","K562A-repro","K562B-repro","MCF7A-repro","MCF7D-repro")
for(j in (1:5)){
  nci.labs[nci.labs == other.labs[j]] <- "Other"
}
```


```{r}
table(nci.labs)
nci.data <- cbind(factor(nci.labs), nci.data)
colnames(nci.data)[1] <- "Labels"
nci.data= nci.data[order(factor(nci.labs)),]
```

```{r}
View(nci.data)
```

```{r}
unique(nci.labs) #14 types of cancer labelled (use this to check the clusters)
#length(unique(nci.labs)) #14
#dim(nci.data) #64 6830
#length(nci.labs) #64
table(nci.labs)
#scaled data
nci.data.s = scale(nci.data[,c(2:6831)])
```


1) 


***
2)

```{r}
sse = rep(0,10)
for (i in 1:10) {
  fit = kmeans(nci.data.s,centers=i)
  sse[i] = fit$totss-fit$betweenss
}
plot(sse,type='b')

r2 = rep(0,10)
for (i in 1:10) {
  fit = kmeans(nci.data.s,centers=i)
  r2[i] = fit$betweenss/fit$totss
}
plot(r2,type='b')
```
Here we are using the sum of squares error (SSE) and $R^2$ for the k-means clustering algorithm on a different number of clusters, 1 through 10. These plots show that as more clusters are added the SSE and $R^2$ measurements become more desirable and also do not appear to level off. Thus we will use 10 clusters on our k-means clustering model, which is also easy to interpret because our nci.labs has 10 unique outcomes (since we combined the 5 categories with each 1 point).

```{r}
set.seed(99)
fitK <- kmeans(nci.data.s, centers=10, nstart = 3)
table(nci.labs.num,fitK$cluster)
```
Now we fit our model on the scaled data, with 10 clusters and also nstart=3 to ensure we find the best set of clusters. In this table we can tell that the clusters returned by the algorithm do not match up with the numbers we assigned to our cancer cell lines, so we must rearrange the table.

```{r, dpi=100, out.height='85%'}
include_graphics("KMeansClusterTable.jpg")
```
This picture shows how we manually assigned clusters as we saw fit to each cancer cell line. Easy decisions included assigning cancer cell line 10 (or RENAL) to cluster 7 and cancer cell line 5 (or MELANOMA) to cluster 2. Eventually we narrowed this down to assign each cancer cell line to a cluster in order to optimize the number of correct assignments.

```{r}
#rearrange the table to get the clusters to correspond in number
nci.labs.num2 = c(8,8,8,8,8,8,8,6,6,6,6,6,10,10,10,10,10,10,10,4,4,4,4,4,4,2,2,2,2,2,2,2,2,5,5,5,5,5,5,5,5,5,3,3,3,3,3,1,1,1,1,1,1,9,9,7,7,7,7,7,7,7,7,7)
table(nci.labs.num2,fitK$cluster)
```

```{r}
#accuracy
sum(nci.labs.num2==fitK$cluster)/64
```

We found an accuracy of 62.5% with assigning the clusters in the way described above. Some interesting findings were that OVARIAN was often grouped in the same cluster as NSCLC and PROSTATE, leading to the decision to name clusters for OVARIAN and PROSTATE that contained less observation than were included in the cluster we deemed the NSCLC one (which was cluster 5). Also the Other category was divided into 3 different clusters, so if we were to perform this k-means clustering differently, another procedure worth testing would be to group the two K562 observations into one, the two MCF7 observations into one, and leave the UNKNOWN as its own cluster. Otherwise our k-means clustering algorithm appeared to work well. 
No clustered plots were included because there were 6830 gene expressions and plotting them all against each other would be impossible to read, and also rather useless because visualizing just two of the many gene expressions would give little information towards the clusters we found.

```{r}
#Other information
fitK$totss #total sum of squares
fitK$betweenss #between cluster sum of squares
fitK$tot.withinss #within cluster sum of squares
```
This additional information shows that the total sum of squares is contributed to more by within cluster sum of squares, indicating that our clusters are large in size and the differences within clusters is actually comparable to the differences between the clusters. The betweenss divided by the totalss was smaller than we saw in the lab with the iris data, which contributes to the explanation of why the accuracy here is lower than that for the iris data.

***
3)


***
4)

***
5)


***
6)


***
BONUS:


***

## Useful Code
```{r eval=FALSE}
#lab 10.6 may help
#import data
library(ISLR)
dim(NCI60)
nci.labs=NCI60$labs
unique(nci.labs) #14 types of cancer labelled (use this to check the clusters)
length(unique(nci.labs)) #14
nci.data=NCI60$data
dim(nci.data) #64 6830
length(nci.labs) #64
table(nci.labs)
#scaled data
nci.data.s = scale(nci.data)
#to assign a color to each of the cancer type so 
#you can use `col = Cols(nci .labs)` in plot argument
Cols= function(vec){
 cols= rainbow(length(unique(vec)))
 return (cols[as.numeric(as.factor(vec))])
}
#Mahalanobis distance formula/function here

#######Don't use the code below directly in submissions.Develop own and justify.
#####Clustering: Please try own directions and improve this code. ##1.PC
dev.off()
pr.out=prcomp(nci.data, scale=TRUE)
summary(pr.out)
plot(pr.out)
#Plot percentages
pve=100*pr.out$sdev^2/sum(pr.out$sdev^2)
par(mfrow=c(1,2))
plot(pve,  type="o", ylab="PVE", xlab="Principal Component", col="blue")
plot(cumsum(pve), type="o", ylab="Cumulative PVE", xlab="Principal Component", col="brown3")
#see first 32 PCs get 80%
cumsum(pve)
#Color the cancer types
Cols=function(vec){
    cols=rainbow(length(unique(vec)))
    return(cols[as.numeric(as.factor(vec))])
  }
#
#Plot PC1, PC2, PC3 with true cancer types: do these cluster and do good job? 
par(mfrow=c(1,2))
plot(pr.out$x[,1:2], col=Cols(nci.labs), pch=19,xlab="Z1",ylab="Z2")
plot(pr.out$x[,c(1,3)], col=Cols(nci.labs), pch=19,xlab="Z1",ylab="Z3")
#These PCs take account of at most 20%. Get insights.

#obtain loadings and interpret:
hist(as.vector(pr.out$rotation[,1])) #PC1 loadings

#biplot? be careful you don't wanna see all columns. i wouldn't use biplot, too densed columns
biplot(...,choices=c(1,2)) #PC1 vs PC2

#make clusters on observations and check?
#make clusters on features and check?

##2.Clustering the Observations of the NCI60 Data
par(mfrow=c(1,3)) #nci.data.s is sclaed data
#row-wise Euclidean distance on scaled data
data.dist=dist(nci.data.s)
#plot the clusters using complete, aver, sign
plot(hclust(data.dist), labels=nci.labs, main="Complete Linkage", xlab="", sub="",ylab="")
plot(hclust(data.dist, method="average"), labels=nci.labs, main="Average Linkage", xlab="", sub="",ylab="")
plot(hclust(data.dist, method="single"), labels=nci.labs,  main="Single Linkage", xlab="", sub="",ylab="")
#Let's cut
hc.out=hclust(dist(nci.data.s))
hc.out
plot(hc.out, labels=nci.labs) #hang = -1
abline(h=139, col="pink")
rect.hclust(hc.out, k = 4, border = "red") 
rect.hclust(hc.out, k = 14, border = "blue") 
#cut at 4 (or 4:14) and see the majority of cancer types
hc.clusters=cutree(hc.out,4) #try 14?
table(hc.clusters,nci.labs)
#which cancer clustered is majority, give this same cancer name to the cluster. then obtain accuracy

#make clusters on observations and check?
#make clusters on features and check?

##3.KMeans
set.seed(99)
#try cluster 4:14
km.out=kmeans(nci.data.s, centers = 6, nstart=20)
km.clusters=km.out$cluster
km.clusters
#compare the clusters to true labels to check: interpret
cbind(km.clusters, nci.labs)
table(km.clusters,nci.labs)
#which cancer clustered is majority, give this same cancer name to the cluster. then obtain accuracy
sum(??==nci.labs)/length(nci.labs)

#How agree hc to km
table(km.clusters,hc.clusters)
hc.out=hclust(dist(pr.out$x[,1:5]))
plot(hc.out, labels=nci.labs, main="Hier. Clust. on First Five Score Vectors")
table(cutree(hc.out,4), nci.labs)
#make clusters on observations and check?
#make clusters on features and check?

##Use mix of methods and make clusters
```


### Write comments, questions: ...


***
I hereby write and submit my solutions without violating the academic honesty and integrity. If not, I accept the consequences. 

### List the fiends you worked with (name, last name): ...

### Disclose the resources or persons if you get any help: ...

### How long did the assignment solutions take?: ...


***
## References
...
