---
title: "Module 9 Assignment on Unsupervised Methods: PC and Clustering"
author: "First Name + Last Name // Undergraduate/Graduate Student"
date: "Today's date"
#output: pdf_document
output:
  pdf_document: default
  df_print: paged
  #html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=80))
```

***

## Module Assignment Questions
## *Applications*

You will perform four unsupervised methods on a high dimensional data: `PCA`, `K-Means` clustering, `hierarchical` clustering, and one of `DBSCAN` and `GMM` clusterings. The data is the `NCI60` cancer cell line microarray data set, which consists of 6,830 gene expression measurements on 64 cancer cell lines. Each cell line is labeled with a cancer type: there is 14 imbalanced types. In performing unsupervised methods, we don't use labels. But after performing the clustering, we can check to see the extent to which these cancer types agree with the results of these unsupervised techniques. You will do this as well.

Do scaling before performing any unsupervised methods. Then, apply each method by justifying how you use and by including informative plots and summaries: each method has parameters and hyperparameters, consider these. Show how you decide optimal numbers of clusters. There is no unique answer key: any decision should be justified as long as you reflect our lab discussions and details. Don't include irrelevant and uncommented results. Make write-ups and outputs readable and compact. Include only necessary codes and outputs in minimalist format.

Fit the four models (1-4) below by including narratives in terms of data reduction and clustering, and answer the questions (5, 6):

1. PCA

2. K-Means

3. Hierarchical

4. DBSCAN or GMM

5. Do the comparison of the four methods above in a table by fitted clusters and true clusters: did clustering methods discover correct clusters? Include an accuracy table or a graph that compares the results obtained from the four methods. Comment.

6. What insights/contextual conclusions did you get about the data from the PCA application? Explain. (this may overlap with your PCA narratives, here, be more contextual on the results of PCA in determining clusters of cancer types.)

BONUS. Use any `manifold` method to cluster the data in terms of cancer types. Then check with the true labels. Does it discover? Explain and include graphs.

***
\newpage

## Your Solutions
```{r}
#install.packages("tidyverse")


```

```{r}
#library(tidyverse)
#library(formattable)
library(MASS)
library(ISLR)
```

```{r}
attach(NCI60)
```


```{r}
summary(NCI60)
View(NCI60$data)
View(NCI60$labs)
```

```{r}
nci.labs=NCI60$labs
nci.data=NCI60$data
```

```{r}
nci.labs=NCI60$labs
```

```{r}
other.labs <- c("UNKNOWN","K562A-repro","K562B-repro","MCF7A-repro","MCF7D-repro")
other.labs
for(j in (1:5)){
  nci.labs[nci.labs == other.labs[j]] <- "Other"
}

```


```{r}
table(nci.labs)
nci.data <- cbind(factor(nci.labs), nci.data)
colnames(nci.data)[1] <- "Labels"
```
```{r}
View(nci.data)
```

```{r}
unique(nci.labs) #14 types of cancer labelled (use this to check the clusters)
#length(unique(nci.labs)) #14
#dim(nci.data) #64 6830
#length(nci.labs) #64
table(nci.labs)
#scaled data
nci.data.s = scale(nci.data)
```

```{r}
new.data = nci.data[-c(1:ncol(nci.data))]
for (i in order(factor(nci.labs))) {
  new.data = rbind(new.data, nci.data[i,])
}
View(new.data)
```

1) 


***
2)


***
3)


***
4)

***
5)


***
6)


***
BONUS:


***

## Useful Code
```{r eval=FALSE}
#lab 10.6 may help
#import data
library(ISLR)
dim(NCI60)
nci.labs=NCI60$labs
unique(nci.labs) #14 types of cancer labelled (use this to check the clusters)
length(unique(nci.labs)) #14
nci.data=NCI60$data
dim(nci.data) #64 6830
length(nci.labs) #64
table(nci.labs)
#scaled data
nci.data.s = scale(nci.data)
#to assign a color to each of the cancer type so 
#you can use `col = Cols(nci .labs)` in plot argument
Cols= function(vec){
 cols= rainbow(length(unique(vec)))
 return (cols[as.numeric(as.factor(vec))])
}
#Mahalanobis distance formula/function here

#######Don't use the code below directly in submissions.Develop own and justify.
#####Clustering: Please try own directions and improve this code. ##1.PC
dev.off()
pr.out=prcomp(nci.data, scale=TRUE)
summary(pr.out)
plot(pr.out)
#Plot percentages
pve=100*pr.out$sdev^2/sum(pr.out$sdev^2)
par(mfrow=c(1,2))
plot(pve,  type="o", ylab="PVE", xlab="Principal Component", col="blue")
plot(cumsum(pve), type="o", ylab="Cumulative PVE", xlab="Principal Component", col="brown3")
#see first 32 PCs get 80%
cumsum(pve)
#Color the cancer types
Cols=function(vec){
    cols=rainbow(length(unique(vec)))
    return(cols[as.numeric(as.factor(vec))])
  }
#
#Plot PC1, PC2, PC3 with true cancer types: do these cluster and do good job? 
par(mfrow=c(1,2))
plot(pr.out$x[,1:2], col=Cols(nci.labs), pch=19,xlab="Z1",ylab="Z2")
plot(pr.out$x[,c(1,3)], col=Cols(nci.labs), pch=19,xlab="Z1",ylab="Z3")
#These PCs take account of at most 20%. Get insights.

#obtain loadings and interpret:
hist(as.vector(pr.out$rotation[,1])) #PC1 loadings

#biplot? be careful you don't wanna see all columns. i wouldn't use biplot, too densed columns
biplot(...,choices=c(1,2)) #PC1 vs PC2

#make clusters on observations and check?
#make clusters on features and check?

##2.Clustering the Observations of the NCI60 Data
par(mfrow=c(1,3)) #nci.data.s is sclaed data
#row-wise Euclidean distance on scaled data
data.dist=dist(nci.data.s)
#plot the clusters using complete, aver, sign
plot(hclust(data.dist), labels=nci.labs, main="Complete Linkage", xlab="", sub="",ylab="")
plot(hclust(data.dist, method="average"), labels=nci.labs, main="Average Linkage", xlab="", sub="",ylab="")
plot(hclust(data.dist, method="single"), labels=nci.labs,  main="Single Linkage", xlab="", sub="",ylab="")
#Let's cut
hc.out=hclust(dist(nci.data.s))
hc.out
plot(hc.out, labels=nci.labs) #hang = -1
abline(h=139, col="pink")
rect.hclust(hc.out, k = 4, border = "red") 
rect.hclust(hc.out, k = 14, border = "blue") 
#cut at 4 (or 4:14) and see the majority of cancer types
hc.clusters=cutree(hc.out,4) #try 14?
table(hc.clusters,nci.labs)
#which cancer clustered is majority, give this same cancer name to the cluster. then obtain accuracy

#make clusters on observations and check?
#make clusters on features and check?

##3.KMeans
set.seed(99)
#try cluster 4:14
km.out=kmeans(nci.data.s, centers = 6, nstart=20)
km.clusters=km.out$cluster
km.clusters
#compare the clusters to true labels to check: interpret
cbind(km.clusters, nci.labs)
table(km.clusters,nci.labs)
#which cancer clustered is majority, give this same cancer name to the cluster. then obtain accuracy
sum(??==nci.labs)/length(nci.labs)

#How agree hc to km
table(km.clusters,hc.clusters)
hc.out=hclust(dist(pr.out$x[,1:5]))
plot(hc.out, labels=nci.labs, main="Hier. Clust. on First Five Score Vectors")
table(cutree(hc.out,4), nci.labs)
#make clusters on observations and check?
#make clusters on features and check?

##Use mix of methods and make clusters
```


### Write comments, questions: ...


***
I hereby write and submit my solutions without violating the academic honesty and integrity. If not, I accept the consequences. 

### List the fiends you worked with (name, last name): ...

### Disclose the resources or persons if you get any help: ...

### How long did the assignment solutions take?: ...


***
## References
...
